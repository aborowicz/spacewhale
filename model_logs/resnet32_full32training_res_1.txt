/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Tue May  7 18:39:57 EDT 2019
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet32 Your trained model will be named resnet32_full32_lr1
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaaf73d6518>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 0.6763 Acc: 0.6489 Err: 0.3511
TP: 4458.0000  TN: 3682.0000  FP: 2621.0000  FN: 1784.0000
Epoch 1/23
----------
train Loss: 0.5449 Acc: 0.7314 Err: 0.2686
TP: 5486.0000  TN: 3690.0000  FP: 2522.0000  FN: 847.0000
Epoch 2/23
----------
train Loss: 0.4946 Acc: 0.7682 Err: 0.2318
TP: 5598.0000  TN: 4039.0000  FP: 2278.0000  FN: 630.0000
Epoch 3/23
----------
train Loss: 0.4455 Acc: 0.7992 Err: 0.2008
TP: 5665.0000  TN: 4361.0000  FP: 2002.0000  FN: 517.0000
Epoch 4/23
----------
train Loss: 0.4424 Acc: 0.7982 Err: 0.2018
TP: 5707.0000  TN: 4306.0000  FP: 1984.0000  FN: 548.0000
Epoch 5/23
----------
train Loss: 0.4278 Acc: 0.8111 Err: 0.1889
TP: 5727.0000  TN: 4448.0000  FP: 1888.0000  FN: 482.0000
Epoch 6/23
----------
train Loss: 0.4198 Acc: 0.8100 Err: 0.1900
TP: 5871.0000  TN: 4291.0000  FP: 1917.0000  FN: 466.0000
Epoch 7/23
----------
train Loss: 0.3707 Acc: 0.8395 Err: 0.1605
TP: 5921.0000  TN: 4610.0000  FP: 1653.0000  FN: 361.0000
Epoch 8/23
----------
train Loss: 0.3603 Acc: 0.8439 Err: 0.1561
TP: 5984.0000  TN: 4603.0000  FP: 1617.0000  FN: 341.0000
Epoch 9/23
----------
train Loss: 0.3541 Acc: 0.8473 Err: 0.1527
TP: 5953.0000  TN: 4676.0000  FP: 1577.0000  FN: 339.0000
Epoch 10/23
----------
train Loss: 0.3510 Acc: 0.8498 Err: 0.1502
TP: 5974.0000  TN: 4687.0000  FP: 1559.0000  FN: 325.0000
Epoch 11/23
----------
train Loss: 0.3498 Acc: 0.8442 Err: 0.1558
TP: 5926.0000  TN: 4665.0000  FP: 1617.0000  FN: 337.0000
Epoch 12/23
----------
train Loss: 0.3433 Acc: 0.8528 Err: 0.1472
TP: 6022.0000  TN: 4676.0000  FP: 1548.0000  FN: 299.0000
Epoch 13/23
----------
train Loss: 0.3480 Acc: 0.8478 Err: 0.1522
TP: 5852.0000  TN: 4784.0000  FP: 1564.0000  FN: 345.0000
Epoch 14/23
----------
train Loss: 0.3419 Acc: 0.8469 Err: 0.1531
TP: 5841.0000  TN: 4783.0000  FP: 1544.0000  FN: 377.0000
Epoch 15/23
----------
train Loss: 0.3422 Acc: 0.8530 Err: 0.1470
TP: 5996.0000  TN: 4705.0000  FP: 1492.0000  FN: 352.0000
Epoch 16/23
----------
train Loss: 0.3427 Acc: 0.8528 Err: 0.1472
TP: 6006.0000  TN: 4693.0000  FP: 1519.0000  FN: 327.0000
Epoch 17/23
----------
train Loss: 0.3328 Acc: 0.8576 Err: 0.1424
TP: 5935.0000  TN: 4824.0000  FP: 1458.0000  FN: 328.0000
Epoch 18/23
----------
train Loss: 0.3381 Acc: 0.8540 Err: 0.1460
TP: 5934.0000  TN: 4780.0000  FP: 1504.0000  FN: 327.0000
Epoch 19/23
----------
train Loss: 0.3331 Acc: 0.8610 Err: 0.1390
TP: 6014.0000  TN: 4787.0000  FP: 1433.0000  FN: 311.0000
Epoch 20/23
----------
train Loss: 0.3394 Acc: 0.8550 Err: 0.1450
TP: 5910.0000  TN: 4816.0000  FP: 1502.0000  FN: 317.0000
Epoch 21/23
----------
train Loss: 0.3341 Acc: 0.8574 Err: 0.1426
TP: 6075.0000  TN: 4681.0000  FP: 1451.0000  FN: 338.0000
Epoch 22/23
----------
train Loss: 0.3456 Acc: 0.8485 Err: 0.1515
TP: 5983.0000  TN: 4662.0000  FP: 1557.0000  FN: 343.0000
Epoch 23/23
----------
train Loss: 0.3336 Acc: 0.8552 Err: 0.1448
TP: 5941.0000  TN: 4787.0000  FP: 1510.0000  FN: 307.0000
-----------------------------------------------------------
Training complete in 122m 27s
-----------------------------------------------------------
Tue May  7 20:42:46 EDT 2019
