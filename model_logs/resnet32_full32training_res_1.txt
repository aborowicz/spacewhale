/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Thu May  9 21:22:30 EDT 2019
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet32 Your trained model will be named resnet32_full32_lr1
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaab31059b0>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 0.8819 Acc: 0.5075 Err: 0.4925
TP: 3228.0000  TN: 3138.0000  FP: 3103.0000  FN: 3076.0000
Epoch 1/23
----------
train Loss: 0.7063 Acc: 0.5386 Err: 0.4614
TP: 3721.0000  TN: 3036.0000  FP: 3175.0000  FN: 2613.0000
Epoch 2/23
----------
train Loss: 0.6894 Acc: 0.5640 Err: 0.4360
TP: 3878.0000  TN: 3198.0000  FP: 3109.0000  FN: 2360.0000
Epoch 3/23
----------
train Loss: 0.6574 Acc: 0.6112 Err: 0.3888
TP: 4403.0000  TN: 3264.0000  FP: 3029.0000  FN: 1849.0000
Epoch 4/23
----------
train Loss: 0.6482 Acc: 0.6269 Err: 0.3731
TP: 4503.0000  TN: 3361.0000  FP: 2965.0000  FN: 1716.0000
Epoch 5/23
----------
train Loss: 0.6295 Acc: 0.6391 Err: 0.3609
TP: 4584.0000  TN: 3434.0000  FP: 2892.0000  FN: 1635.0000
Epoch 6/23
----------
train Loss: 0.6090 Acc: 0.6620 Err: 0.3380
TP: 4810.0000  TN: 3495.0000  FP: 2840.0000  FN: 1400.0000
Epoch 7/23
----------
train Loss: 0.5719 Acc: 0.7012 Err: 0.2988
TP: 5447.0000  TN: 3350.0000  FP: 3071.0000  FN: 677.0000
Epoch 8/23
----------
train Loss: 0.5671 Acc: 0.7039 Err: 0.2961
TP: 5311.0000  TN: 3520.0000  FP: 2945.0000  FN: 769.0000
Epoch 9/23
----------
train Loss: 0.5600 Acc: 0.7112 Err: 0.2888
TP: 5572.0000  TN: 3350.0000  FP: 2885.0000  FN: 738.0000
Epoch 10/23
----------
train Loss: 0.5512 Acc: 0.7168 Err: 0.2832
TP: 5447.0000  TN: 3545.0000  FP: 2769.0000  FN: 784.0000
Epoch 11/23
----------
train Loss: 0.5595 Acc: 0.7122 Err: 0.2878
TP: 5566.0000  TN: 3368.0000  FP: 2888.0000  FN: 723.0000
Epoch 12/23
----------
train Loss: 0.5531 Acc: 0.7185 Err: 0.2815
TP: 5495.0000  TN: 3518.0000  FP: 2804.0000  FN: 728.0000
Epoch 13/23
----------
train Loss: 0.5531 Acc: 0.7184 Err: 0.2816
TP: 5473.0000  TN: 3539.0000  FP: 2773.0000  FN: 760.0000
Epoch 14/23
----------
train Loss: 0.5485 Acc: 0.7216 Err: 0.2784
TP: 5587.0000  TN: 3465.0000  FP: 2801.0000  FN: 692.0000
Epoch 15/23
----------
train Loss: 0.5362 Acc: 0.7313 Err: 0.2687
TP: 5622.0000  TN: 3552.0000  FP: 2711.0000  FN: 660.0000
Epoch 16/23
----------
train Loss: 0.5411 Acc: 0.7243 Err: 0.2757
TP: 5527.0000  TN: 3559.0000  FP: 2745.0000  FN: 714.0000
Epoch 17/23
----------
train Loss: 0.5445 Acc: 0.7216 Err: 0.2784
TP: 5546.0000  TN: 3507.0000  FP: 2775.0000  FN: 717.0000
Epoch 18/23
----------
train Loss: 0.5416 Acc: 0.7224 Err: 0.2776
TP: 5576.0000  TN: 3487.0000  FP: 2765.0000  FN: 717.0000
Epoch 19/23
----------
train Loss: 0.5392 Acc: 0.7297 Err: 0.2703
TP: 5548.0000  TN: 3606.0000  FP: 2700.0000  FN: 691.0000
Epoch 20/23
----------
train Loss: 0.5412 Acc: 0.7273 Err: 0.2727
TP: 5617.0000  TN: 3507.0000  FP: 2729.0000  FN: 692.0000
Epoch 21/23
----------
train Loss: 0.5364 Acc: 0.7284 Err: 0.2716
TP: 5672.0000  TN: 3466.0000  FP: 2739.0000  FN: 668.0000
Epoch 22/23
----------
train Loss: 0.5472 Acc: 0.7188 Err: 0.2812
TP: 5499.0000  TN: 3518.0000  FP: 2828.0000  FN: 700.0000
Epoch 23/23
----------
train Loss: 0.5446 Acc: 0.7240 Err: 0.2760
TP: 5586.0000  TN: 3496.0000  FP: 2731.0000  FN: 732.0000
-----------------------------------------------------------
Training complete in 124m 58s
-----------------------------------------------------------
Thu May  9 23:28:22 EDT 2019
