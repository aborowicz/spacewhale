/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Thu May  9 12:30:56 EDT 2019
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet32 Your trained model will be named resnet32_full32_lr2
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaaf5413fd0>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 0.9623 Acc: 0.5031 Err: 0.4969
TP: 3088.0000  TN: 3224.0000  FP: 3088.0000  FN: 3145.0000
Epoch 1/23
----------
train Loss: 0.7556 Acc: 0.5079 Err: 0.4921
TP: 3111.0000  TN: 3261.0000  FP: 3039.0000  FN: 3134.0000
Epoch 2/23
----------
train Loss: 0.7465 Acc: 0.5120 Err: 0.4880
TP: 3382.0000  TN: 3041.0000  FP: 3170.0000  FN: 2952.0000
Epoch 3/23
----------
train Loss: 0.7391 Acc: 0.5437 Err: 0.4563
TP: 3654.0000  TN: 3167.0000  FP: 3077.0000  FN: 2647.0000
Epoch 4/23
----------
train Loss: 0.7261 Acc: 0.5541 Err: 0.4459
TP: 3796.0000  TN: 3155.0000  FP: 3061.0000  FN: 2533.0000
Epoch 5/23
----------
train Loss: 0.7123 Acc: 0.5604 Err: 0.4396
TP: 3780.0000  TN: 3250.0000  FP: 3012.0000  FN: 2503.0000
Epoch 6/23
----------
train Loss: 0.7048 Acc: 0.5656 Err: 0.4344
TP: 3684.0000  TN: 3412.0000  FP: 2957.0000  FN: 2492.0000
Epoch 7/23
----------
train Loss: 0.6446 Acc: 0.5966 Err: 0.4034
TP: 5090.0000  TN: 2394.0000  FP: 3901.0000  FN: 1160.0000
Epoch 8/23
----------
train Loss: 0.6332 Acc: 0.6158 Err: 0.3842
TP: 5270.0000  TN: 2455.0000  FP: 3838.0000  FN: 982.0000
Epoch 9/23
----------
train Loss: 0.6385 Acc: 0.6137 Err: 0.3863
TP: 5368.0000  TN: 2331.0000  FP: 3874.0000  FN: 972.0000
Epoch 10/23
----------
train Loss: 0.6276 Acc: 0.6282 Err: 0.3718
TP: 5630.0000  TN: 2251.0000  FP: 3940.0000  FN: 724.0000
Epoch 11/23
----------
train Loss: 0.6260 Acc: 0.6220 Err: 0.3780
TP: 5241.0000  TN: 2562.0000  FP: 3740.0000  FN: 1002.0000
Epoch 12/23
----------
train Loss: 0.6281 Acc: 0.6248 Err: 0.3752
TP: 5399.0000  TN: 2439.0000  FP: 3859.0000  FN: 848.0000
Epoch 13/23
----------
train Loss: 0.6247 Acc: 0.6311 Err: 0.3689
TP: 5446.0000  TN: 2471.0000  FP: 3775.0000  FN: 853.0000
Epoch 14/23
----------
train Loss: 0.6238 Acc: 0.6279 Err: 0.3721
TP: 5649.0000  TN: 2228.0000  FP: 4114.0000  FN: 554.0000
Epoch 15/23
----------
train Loss: 0.6202 Acc: 0.6317 Err: 0.3683
TP: 5608.0000  TN: 2317.0000  FP: 4032.0000  FN: 588.0000
Epoch 16/23
----------
train Loss: 0.6196 Acc: 0.6356 Err: 0.3644
TP: 5807.0000  TN: 2167.0000  FP: 4072.0000  FN: 499.0000
Epoch 17/23
----------
train Loss: 0.6143 Acc: 0.6371 Err: 0.3629
TP: 5845.0000  TN: 2148.0000  FP: 4094.0000  FN: 458.0000
Epoch 18/23
----------
train Loss: 0.6140 Acc: 0.6434 Err: 0.3566
TP: 5794.0000  TN: 2277.0000  FP: 4010.0000  FN: 464.0000
Epoch 19/23
----------
train Loss: 0.6195 Acc: 0.6356 Err: 0.3644
TP: 5885.0000  TN: 2088.0000  FP: 4141.0000  FN: 431.0000
Epoch 20/23
----------
train Loss: 0.6157 Acc: 0.6368 Err: 0.3632
TP: 5606.0000  TN: 2383.0000  FP: 3948.0000  FN: 608.0000
Epoch 21/23
----------
train Loss: 0.6147 Acc: 0.6343 Err: 0.3657
TP: 5474.0000  TN: 2483.0000  FP: 3881.0000  FN: 707.0000
Epoch 22/23
----------
train Loss: 0.6175 Acc: 0.6375 Err: 0.3625
TP: 5743.0000  TN: 2254.0000  FP: 4001.0000  FN: 547.0000
Epoch 23/23
----------
train Loss: 0.6106 Acc: 0.6464 Err: 0.3536
TP: 5825.0000  TN: 2284.0000  FP: 3952.0000  FN: 484.0000
-----------------------------------------------------------
Training complete in 124m 34s
-----------------------------------------------------------
Thu May  9 14:36:03 EDT 2019
