/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Mon May 20 12:58:10 EDT 2019
Training. Resnet152. LR=0.01
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet152 Your trained model will be named resnet152_01
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaaf73d6400>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 0.7237 Acc: 0.5251 Err: 0.4748
TP: 3808.0000  TN: 2780.0000  FP: 3447.0000  FN: 2509.0000
Epoch 1/23
----------
train Loss: 0.6625 Acc: 0.5801 Err: 0.4198
TP: 4676.0000  TN: 2601.0000  FP: 3743.0000  FN: 1524.0000
Epoch 2/23
----------
train Loss: 0.6350 Acc: 0.6238 Err: 0.3762
TP: 5191.0000  TN: 2634.0000  FP: 3683.0000  FN: 1036.0000
Epoch 3/23
----------
train Loss: 0.6729 Acc: 0.5637 Err: 0.4363
TP: 4518.0000  TN: 2553.0000  FP: 3653.0000  FN: 1820.0000
Epoch 4/23
----------
train Loss: 0.6797 Acc: 0.5553 Err: 0.4446
TP: 4366.0000  TN: 2600.0000  FP: 3695.0000  FN: 1883.0000
Epoch 5/23
----------
train Loss: 0.6794 Acc: 0.5465 Err: 0.4534
TP: 4584.0000  TN: 2272.0000  FP: 3914.0000  FN: 1774.0000
Epoch 6/23
----------
train Loss: 0.6580 Acc: 0.5954 Err: 0.4045
TP: 5161.0000  TN: 2308.0000  FP: 3890.0000  FN: 1185.0000
Epoch 7/23
----------
train Loss: 0.6326 Acc: 0.6292 Err: 0.3707
TP: 5428.0000  TN: 2465.0000  FP: 3820.0000  FN: 831.0000
Epoch 8/23
----------
train Loss: 0.6201 Acc: 0.6466 Err: 0.3534
TP: 5452.0000  TN: 2659.0000  FP: 3652.0000  FN: 781.0000
Epoch 9/23
----------
train Loss: 0.6136 Acc: 0.6574 Err: 0.3425
TP: 5478.0000  TN: 2769.0000  FP: 3505.0000  FN: 792.0000
Epoch 10/23
----------
train Loss: 0.5982 Acc: 0.6750 Err: 0.3249
TP: 5592.0000  TN: 2876.0000  FP: 3358.0000  FN: 718.0000
Epoch 11/23
----------
train Loss: 0.5889 Acc: 0.6809 Err: 0.3190
TP: 5522.0000  TN: 3020.0000  FP: 3294.0000  FN: 708.0000
Epoch 12/23
----------
train Loss: 0.5796 Acc: 0.6995 Err: 0.3004
TP: 5631.0000  TN: 3144.0000  FP: 3052.0000  FN: 717.0000
Epoch 13/23
----------
train Loss: 0.5684 Acc: 0.7052 Err: 0.2947
TP: 5616.0000  TN: 3231.0000  FP: 2994.0000  FN: 703.0000
Epoch 14/23
----------
train Loss: 0.5540 Acc: 0.7182 Err: 0.2817
TP: 5679.0000  TN: 3331.0000  FP: 2892.0000  FN: 642.0000
Epoch 15/23
----------
train Loss: 0.5456 Acc: 0.7244 Err: 0.2755
TP: 5590.0000  TN: 3498.0000  FP: 2731.0000  FN: 725.0000
Epoch 16/23
----------
train Loss: 0.5458 Acc: 0.7210 Err: 0.2789
TP: 5538.0000  TN: 3507.0000  FP: 2786.0000  FN: 713.0000
Epoch 17/23
----------
train Loss: 0.5297 Acc: 0.7397 Err: 0.2602
TP: 5570.0000  TN: 3710.0000  FP: 2555.0000  FN: 709.0000
Epoch 18/23
----------
train Loss: 0.5265 Acc: 0.7402 Err: 0.2597
TP: 5638.0000  TN: 3648.0000  FP: 2578.0000  FN: 680.0000
Epoch 19/23
----------
train Loss: 0.5228 Acc: 0.7362 Err: 0.2637
TP: 5412.0000  TN: 3824.0000  FP: 2548.0000  FN: 760.0000
Epoch 20/23
----------
train Loss: 0.5239 Acc: 0.7451 Err: 0.2548
TP: 5661.0000  TN: 3686.0000  FP: 2509.0000  FN: 688.0000
Epoch 21/23
----------
train Loss: 0.5144 Acc: 0.7465 Err: 0.2534
TP: 5549.0000  TN: 3816.0000  FP: 2477.0000  FN: 702.0000
Epoch 22/23
----------
train Loss: 0.5155 Acc: 0.7506 Err: 0.2493
TP: 5661.0000  TN: 3755.0000  FP: 2443.0000  FN: 685.0000
Epoch 23/23
----------
train Loss: 0.5146 Acc: 0.7447 Err: 0.2552
TP: 5504.0000  TN: 3838.0000  FP: 2429.0000  FN: 773.0000
-----------------------------------------------------------
Training complete in 414m 33s
-----------------------------------------------------------
Mon May 20 19:53:24 EDT 2019
