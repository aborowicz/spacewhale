/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Mon May  6 09:19:28 EDT 2019
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet18 Your trained model will be named resnet18_full32_lr001
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaafcaeb6a0>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 0.7465 Acc: 0.6151 Err: 0.3849
TP: 4226.0000  TN: 3491.0000  FP: 2765.0000  FN: 2063.0000
Epoch 1/23
----------
train Loss: 0.5753 Acc: 0.7187 Err: 0.2813
TP: 5346.0000  TN: 3670.0000  FP: 2533.0000  FN: 996.0000
Epoch 2/23
----------
train Loss: 0.5124 Acc: 0.7574 Err: 0.2426
TP: 5513.0000  TN: 3989.0000  FP: 2282.0000  FN: 761.0000
Epoch 3/23
----------
train Loss: 0.4698 Acc: 0.7851 Err: 0.2149
TP: 5643.0000  TN: 4206.0000  FP: 2045.0000  FN: 651.0000
Epoch 4/23
----------
train Loss: 0.4461 Acc: 0.8005 Err: 0.1995
TP: 5676.0000  TN: 4366.0000  FP: 1943.0000  FN: 560.0000
Epoch 5/23
----------
train Loss: 0.4266 Acc: 0.8121 Err: 0.1879
TP: 5867.0000  TN: 4321.0000  FP: 1854.0000  FN: 503.0000
Epoch 6/23
----------
train Loss: 0.4317 Acc: 0.8084 Err: 0.1916
TP: 5713.0000  TN: 4429.0000  FP: 1885.0000  FN: 518.0000
Epoch 7/23
----------
train Loss: 0.4016 Acc: 0.8210 Err: 0.1790
TP: 5787.0000  TN: 4513.0000  FP: 1786.0000  FN: 459.0000
Epoch 8/23
----------
train Loss: 0.3685 Acc: 0.8395 Err: 0.1605
TP: 5787.0000  TN: 4744.0000  FP: 1611.0000  FN: 403.0000
Epoch 9/23
----------
train Loss: 0.3738 Acc: 0.8356 Err: 0.1644
TP: 5861.0000  TN: 4622.0000  FP: 1643.0000  FN: 419.0000
Epoch 10/23
----------
train Loss: 0.3740 Acc: 0.8366 Err: 0.1634
TP: 5864.0000  TN: 4631.0000  FP: 1639.0000  FN: 411.0000
Epoch 11/23
----------
train Loss: 0.3579 Acc: 0.8461 Err: 0.1539
TP: 5952.0000  TN: 4662.0000  FP: 1538.0000  FN: 393.0000
Epoch 12/23
----------
train Loss: 0.3639 Acc: 0.8448 Err: 0.1552
TP: 5990.0000  TN: 4608.0000  FP: 1566.0000  FN: 381.0000
Epoch 13/23
----------
train Loss: 0.3592 Acc: 0.8446 Err: 0.1554
TP: 5866.0000  TN: 4730.0000  FP: 1562.0000  FN: 387.0000
Epoch 14/23
----------
train Loss: 0.3540 Acc: 0.8493 Err: 0.1507
TP: 6009.0000  TN: 4645.0000  FP: 1559.0000  FN: 332.0000
Epoch 15/23
----------
train Loss: 0.3454 Acc: 0.8529 Err: 0.1471
TP: 5843.0000  TN: 4857.0000  FP: 1492.0000  FN: 353.0000
Epoch 16/23
----------
train Loss: 0.3485 Acc: 0.8504 Err: 0.1496
TP: 5914.0000  TN: 4754.0000  FP: 1531.0000  FN: 346.0000
Epoch 17/23
----------
train Loss: 0.3576 Acc: 0.8444 Err: 0.1556
TP: 5933.0000  TN: 4660.0000  FP: 1598.0000  FN: 354.0000
Epoch 18/23
----------
train Loss: 0.3570 Acc: 0.8436 Err: 0.1564
TP: 5849.0000  TN: 4734.0000  FP: 1576.0000  FN: 386.0000
Epoch 19/23
----------
train Loss: 0.3487 Acc: 0.8497 Err: 0.1503
TP: 5868.0000  TN: 4791.0000  FP: 1513.0000  FN: 373.0000
Epoch 20/23
----------
train Loss: 0.3583 Acc: 0.8434 Err: 0.1566
TP: 5946.0000  TN: 4635.0000  FP: 1587.0000  FN: 377.0000
Epoch 21/23
----------
train Loss: 0.3537 Acc: 0.8453 Err: 0.1547
TP: 5882.0000  TN: 4722.0000  FP: 1590.0000  FN: 351.0000
Epoch 22/23
----------
train Loss: 0.3514 Acc: 0.8464 Err: 0.1536
TP: 5950.0000  TN: 4668.0000  FP: 1603.0000  FN: 324.0000
Epoch 23/23
----------
train Loss: 0.3512 Acc: 0.8476 Err: 0.1524
TP: 5859.0000  TN: 4774.0000  FP: 1533.0000  FN: 379.0000
-----------------------------------------------------------
Training complete in 67m 44s
-----------------------------------------------------------
Mon May  6 10:28:12 EDT 2019
