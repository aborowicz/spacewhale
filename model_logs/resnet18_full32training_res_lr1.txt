/gpfs/projects/LynchGroup/spacewhale/git_spacewhale/spacewhale/shell_scripts
/gpfs/projects/LynchGroup/spacewhale
Mon May  6 14:26:12 EDT 2019
######################################################################################################
WELCOME TO SPACEWHALE!
######################################################################################################
We will now train your model.. please be patient
Using resnet18 Your trained model will be named resnet18_full32_lr1
------------------------------------------------------------------------------
<torch.utils.data.dataloader.DataLoader object at 0x2aaafcaebb38>
Your dataset size is: 12545
You have 2 classes in your dataset
------------------------------------------------------------------------------
Labels for the dataset are:
water = 0
whale = 1
------------------------------------------------------------------------------
Data loaded into gpu
------------------------------------------------------------------------------
Epoch 0/23
----------
train Loss: 1.0268 Acc: 0.5207 Err: 0.4793
TP: 3406.0000  TN: 3126.0000  FP: 3098.0000  FN: 2915.0000
Epoch 1/23
----------
train Loss: 0.7115 Acc: 0.5215 Err: 0.4785
TP: 3495.0000  TN: 3047.0000  FP: 3170.0000  FN: 2833.0000
Epoch 2/23
----------
train Loss: 0.6969 Acc: 0.5449 Err: 0.4551
TP: 3724.0000  TN: 3112.0000  FP: 3187.0000  FN: 2522.0000
Epoch 3/23
----------
train Loss: 0.6924 Acc: 0.5488 Err: 0.4512
TP: 3752.0000  TN: 3133.0000  FP: 3201.0000  FN: 2459.0000
Epoch 4/23
----------
train Loss: 0.6971 Acc: 0.5519 Err: 0.4481
TP: 3687.0000  TN: 3236.0000  FP: 3124.0000  FN: 2498.0000
Epoch 5/23
----------
train Loss: 0.6817 Acc: 0.5672 Err: 0.4328
TP: 4383.0000  TN: 2733.0000  FP: 3368.0000  FN: 2061.0000
Epoch 6/23
----------
train Loss: 0.6701 Acc: 0.5724 Err: 0.4276
TP: 4124.0000  TN: 3057.0000  FP: 3167.0000  FN: 2197.0000
Epoch 7/23
----------
train Loss: 0.6432 Acc: 0.6145 Err: 0.3855
TP: 5653.0000  TN: 2056.0000  FP: 4213.0000  FN: 623.0000
Epoch 8/23
----------
train Loss: 0.6351 Acc: 0.6210 Err: 0.3790
TP: 5467.0000  TN: 2324.0000  FP: 3970.0000  FN: 784.0000
Epoch 9/23
----------
train Loss: 0.6336 Acc: 0.6287 Err: 0.3713
TP: 5525.0000  TN: 2362.0000  FP: 3913.0000  FN: 745.0000
Epoch 10/23
----------
train Loss: 0.6218 Acc: 0.6407 Err: 0.3593
TP: 5443.0000  TN: 2594.0000  FP: 3743.0000  FN: 765.0000
Epoch 11/23
----------
train Loss: 0.6140 Acc: 0.6597 Err: 0.3403
TP: 5751.0000  TN: 2525.0000  FP: 3673.0000  FN: 596.0000
Epoch 12/23
----------
train Loss: 0.6134 Acc: 0.6580 Err: 0.3420
TP: 5722.0000  TN: 2533.0000  FP: 3689.0000  FN: 601.0000
Epoch 13/23
----------
train Loss: 0.6064 Acc: 0.6658 Err: 0.3342
TP: 5748.0000  TN: 2604.0000  FP: 3603.0000  FN: 590.0000
Epoch 14/23
----------
train Loss: 0.5981 Acc: 0.6700 Err: 0.3300
TP: 5745.0000  TN: 2660.0000  FP: 3615.0000  FN: 525.0000
Epoch 15/23
----------
train Loss: 0.5998 Acc: 0.6709 Err: 0.3291
TP: 5744.0000  TN: 2673.0000  FP: 3595.0000  FN: 533.0000
Epoch 16/23
----------
train Loss: 0.5986 Acc: 0.6697 Err: 0.3303
TP: 5728.0000  TN: 2674.0000  FP: 3602.0000  FN: 541.0000
Epoch 17/23
----------
train Loss: 0.5980 Acc: 0.6729 Err: 0.3271
TP: 5800.0000  TN: 2642.0000  FP: 3518.0000  FN: 585.0000
Epoch 18/23
----------
train Loss: 0.5971 Acc: 0.6773 Err: 0.3227
TP: 5834.0000  TN: 2663.0000  FP: 3465.0000  FN: 583.0000
Epoch 19/23
----------
train Loss: 0.5932 Acc: 0.6727 Err: 0.3273
TP: 5738.0000  TN: 2701.0000  FP: 3545.0000  FN: 561.0000
Epoch 20/23
----------
train Loss: 0.5950 Acc: 0.6725 Err: 0.3275
TP: 5679.0000  TN: 2757.0000  FP: 3533.0000  FN: 576.0000
Epoch 21/23
----------
train Loss: 0.5888 Acc: 0.6749 Err: 0.3251
TP: 5683.0000  TN: 2783.0000  FP: 3514.0000  FN: 565.0000
Epoch 22/23
----------
train Loss: 0.5914 Acc: 0.6776 Err: 0.3224
TP: 5772.0000  TN: 2728.0000  FP: 3508.0000  FN: 537.0000
Epoch 23/23
----------
train Loss: 0.5920 Acc: 0.6737 Err: 0.3263
TP: 5638.0000  TN: 2813.0000  FP: 3529.0000  FN: 565.0000
-----------------------------------------------------------
Training complete in 67m 31s
-----------------------------------------------------------
Mon May  6 15:35:30 EDT 2019
